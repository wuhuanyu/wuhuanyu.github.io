<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>跟你的女朋友解释PCA</title>
      <link href="/2018/08/09/%E8%B7%9F%E4%BD%A0%E7%9A%84%E5%A5%B3%E6%9C%8B%E5%8F%8B%E8%A7%A3%E9%87%8APCA/"/>
      <url>/2018/08/09/%E8%B7%9F%E4%BD%A0%E7%9A%84%E5%A5%B3%E6%9C%8B%E5%8F%8B%E8%A7%A3%E9%87%8APCA/</url>
      <content type="html"><![CDATA[<p>Note: 本文假定读者和他的女朋友已经了解PCA(Principal Component Analysis)和FA(Factor Analysis)的数学原理，但是读者的女朋友需要一些更直观的理解，因此本文不对数学推导做详细解释。</p><h3 id="PCA的作用？举一些例子？"><a href="#PCA的作用？举一些例子？" class="headerlink" title="PCA的作用？举一些例子？"></a>PCA的作用？举一些例子？</h3><p>不同于LDA(Linear Discriminant Analysis,线性判别分析)PCA是一种无监督的数据降维技术。在原始数据集中，通常属性与属性之间存在相关性，举个例子，在给跑车定价中，一辆跑车的属性有${百公里加速时间，最高时速，颜色，发动机….}$等等，显然百公里加速和最高时速存在一定的关系，也就是存在信息冗余，利用PCA可以降低数据冗余，同时很好的保留原有的数据信息。</p><p>PCA的应用场景有：</p><ol><li>数据压缩</li><li>在进行监督学习中，如果使用PCA对数据进行预处理，除了计算量下降外，有效减轻过拟合现象</li><li>PCA是一种降噪算法，在PCA中，我们一般只关心对数据多样性贡献较大的属性，忽略方差小的属性。</li></ol><h3 id="PCA属于feature-elimination-还是feature-extraction"><a href="#PCA属于feature-elimination-还是feature-extraction" class="headerlink" title="PCA属于feature elimination 还是feature extraction"></a>PCA属于feature elimination 还是feature extraction</h3><p>feature elimination 是直接把属性砍掉，这样虽然降低了数据的维度，但是损失了信息,PCA属于feature extraction，假设原先的数据有$n$个属性，降维后$k$个属性，PCA是将$n$个属性重新线性组合得到$k$个属性，</p><h3 id="PCA属性重组的标准是什么？"><a href="#PCA属性重组的标准是什么？" class="headerlink" title="PCA属性重组的标准是什么？"></a>PCA属性重组的标准是什么？</h3><p>PCA对属性重组有两个基本的标准：</p><ol><li>重组后的属性必须有较大的方差</li><li>重组后的属性必须能够尽可能的恢复原属性</li></ol><p>笔者在查阅资料的时候，思考了这两个标准之间的联系。感谢信息论老师，感谢学习到的信息论知识。下面就从信息论的角度直观地解释一下这两个标准的含义。</p><p>学过信息论就知道，作为衡量系统无序度的熵，如果一组数据方差小，就意味着信息熵小，极端的例子是，如果一组数据一模一样，那么信息熵就就是0，如果一组数据方差大，那么信息熵就大。</p><p>现在我们对数据进行压缩，追求的目标就是信息损失越小越好，如果PCA不满足第一个标准，即重组后的属性方差很小，那么我们就丢掉了大部分信息，这个时候我们不可能对数据进行重建。当然PCA是有损压缩(如果熟悉PCA的推导就知道，最后的变换矩阵不可逆)，但是如果重组后属性方差大，损失的信息就小，越容易重建原数据(当然完全重建是不可能的）</p><p>因此，这两个标准是内在统一的。<br><img src="https://i.stack.imgur.com/Q7HIP.gif"><br>例如上图，蓝点表示二维数据集，红点表示投影点也就是重组后的属性，紫红线表示原数据集方差最大的方向，大红线可以理解为误差，也就是损失的信息。中间的空心点表示均值。当然，你如果仔细看，会发现，重组后的属性方差越大，误差就越小，表明损失越小；重组后的属性方差越小，误差越大，表明损失的信息量太大了。你如果更仔细看会发现，这两个量的和是守恒的，23333</p><h3 id="PCA为什么会涉及到特征值分解"><a href="#PCA为什么会涉及到特征值分解" class="headerlink" title="PCA为什么会涉及到特征值分解"></a>PCA为什么会涉及到特征值分解</h3><p>严格来讲，这涉及到公式推导的问题，在公式推导中有一个求最值的过程，这个时候涉及到特征值分解。当然也可以更加直观的理解，<a href="https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector" target="_blank" rel="noopener">这里</a>有个链接告诉你特征值和特征向量究竟是什么。</p><h3 id="PCA-V-S-FA"><a href="#PCA-V-S-FA" class="headerlink" title="PCA V.S. FA"></a>PCA V.S. FA</h3><p>我发现这个坑还是比较大的，我要仔细思考思考。</p><hr><p>听完了你的解释，你的女朋友若有所思的点点头，连连夸你聪明，觉得你是世界上最厉害的。你也觉得这次分享很有必要，不仅加深了自己对PCA的理解，学习到的各种知识更加融会贯通了。但是此刻你们都觉得很累，毕竟刚刚进行了一场大脑风暴。你搂着女朋友沉沉地睡去了。</p><p>第二天醒来，你发现你并没有女朋友。</p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li><a href="https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector" target="_blank" rel="noopener">https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector</a></li><li><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues</a></li><li>cs229</li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Statistic </tag>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GLM 概览</title>
      <link href="/2018/08/03/GLM-%E6%A6%82%E8%A7%88/"/>
      <url>/2018/08/03/GLM-%E6%A6%82%E8%A7%88/</url>
      <content type="html"><![CDATA[<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><p>本文大部分总结自<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></p><h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>大家一开始接触回归问题的时候一定遇到过简单线性回归，Logistic Regression和Softmax问题，我在学习后两种回归问题的时候对方程的形式感觉很奇怪。比如：Logsitic Regression的方程(省略偏置项)如下：$$P(y=1|x)={1\over{1+e^{-w^Tx}}}$$$$P(y=0|x)={e^{-w^Tx}\over{1+e^{-w^Tx}}}$$为什么会是这种形式？这涉及到通用线性模型的概念(Generalized Linear Model,GLM),从GLM可以回答这个问题。</p><h1 id="GLM及其特化"><a href="#GLM及其特化" class="headerlink" title="GLM及其特化"></a>GLM及其特化</h1><p>GLM模型是建立在指数函数家族之上的,指数家族指的是下面这种形式的函数：$$P(y;\eta)=b(y)e^{\eta^TT(y)-a(\eta)}$$其中，各参数含义如下(这部分我不是特别清楚，从参考文献直接搬过来)：</p><ol><li>$\eta$为自然参数(Natural Parameter)</li><li>$T(y)$为sufficient statistic(下文考虑的分布通常为$T(y)=y$)</li><li>$a(\eta)$为归一化参量，用来使得算出的概率总和为1</li></ol><p>GLM做出如下假设：</p><ol><li>$y|x;\theta$ 服从某指数家族分布</li><li>讨论的目标为$E(y|x)$</li><li>$\eta$和$x$满足线性关系$\eta=w^Tx$</li></ol><p>简单线性回归，Logistic Regression 和Softmax问题都能通过方程变形变为GLM</p><h2 id="简单线性回归（以下简称线性回归，高斯分布）"><a href="#简单线性回归（以下简称线性回归，高斯分布）" class="headerlink" title="简单线性回归（以下简称线性回归，高斯分布）"></a>简单线性回归（以下简称线性回归，高斯分布）</h2><p>我们在处理线性回归问题的时候，通常如下建模：$$y=w^Tx+\epsilon$$其中$\epsilon$为误差项，一般认为$\epsilon\sim N(0,\delta^2)$,因为$\delta$的取值不影响讨论，不妨认为$\delta=1$,因此$y\sim N(w^Tx,1)$,因此$$\begin{eqnarray} p(y|x) &amp;=&amp; {1\over \sqrt{2\pi}}exp{(y-\mu)^2\over 2} \\\  &amp;=&amp; {1\over \sqrt{2\pi}} exp(-{y^2\over2})exp(\mu y-{\mu^2\over2})\end{eqnarray}$$所以$$\eta=\mu=w^Tx$$$$T(y)=y$$$$a(\eta)={\mu^2/2}$$ $$b(y)={1\over \sqrt{2\pi}}exp(-{y^2\over2})$$</p><h2 id="Logistic-Regression-伯努利分布"><a href="#Logistic-Regression-伯努利分布" class="headerlink" title="Logistic Regression(伯努利分布)"></a>Logistic Regression(伯努利分布)</h2><p>通过一定的变形，同样可以将Losgistic Regression 变形为指数分布，不详述。假设$p(y=1;\phi)=\phi$，结果如下<br>$$\eta=ln({\phi\over{1-\phi}})=&gt;\phi={1\over{1+e^{-\eta}}}$$ $$T(y)=y$$ $$a(\eta)=ln(1+e^\eta)$$ $$b(y)=1$$ 这个时候我们就发现，由于上述GLM的假设，自然得出$$P(y=1|x)={1\over{1+e^{-w^Tx}}}$$$$P(y=0|x)={e^{-w^Tx}\over{1+e^{-w^Tx}}}$$<br>这是相当奇妙的，我一开始接触到这个结论的时候很困惑，毕竟从伯努利分布得到Logistic Regression 这种形式还是有一定的跨度的，原来过程就藏在GLM模型里！</p><h3 id="Logistic-Regression-和高斯判定模型的联系"><a href="#Logistic-Regression-和高斯判定模型的联系" class="headerlink" title="Logistic Regression 和高斯判定模型的联系"></a>Logistic Regression 和高斯判定模型的联系</h3><p>此处岔开一段，和本文主题无关。实际上，Logistic Regression和高斯判定模型(Gaussian Discriminant Model)存在一些联系，简而言之，高斯判定模型是Logistic Regression的强化，高斯判定也是一种Logistic Regression。为了说明这种关系，定义符号如下:(假定读者熟悉多维高斯分布)</p><ol><li>$P(y=1)=\phi$</li><li>$\mu=\mu_0^{1-y}\mu_1^y,p(x|y=0,1)\sim e^{-{(x-\mu)^T\sum^{-1}(x-\mu)}\over2}$</li></ol><p>那么，我们要化简的公式就是：$p(y=1|x)$,如下：<br>$$\begin{eqnarray} p(y=1|x) &amp;=&amp;{p(y=1,x)\over p(x)} \\\ &amp;=&amp; {p(x|y=1)p(y=1)\over{\sum_i p(x|y=i)p(y=i)}} \end{eqnarray}$$ 读者将各式子带入并进行化简就能获得Logistic Regression公式的形式，笔者已经推导出来，因此应该是正确的。</p><p>那我们应该选择哪种判别模型呢，因为如果一个模型是高斯判定模型，那么它一定可以用Logistic Regression 进行拟合，反之不成立，因此，如果我们对模型有先验知识，认为该模型是高斯判定，那么就用高斯判定。</p><h2 id="Softmax（多项分布）"><a href="#Softmax（多项分布）" class="headerlink" title="Softmax（多项分布）"></a>Softmax（多项分布）</h2><p>其实仔细思考，伯努利分布其实是多项分布的特化，Logistic Regression 和Softmax的形式也有相似之处。由多项分布变形成GLM并不复杂，但是要借助一个小trick，即one hot编码，为了节省我的工作量，我就不写啦。详情见<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">26</a></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> GLM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Some tips on GradientBoosting</title>
      <link href="/2018/07/31/Some-tips-on-GradientBoosting/"/>
      <url>/2018/07/31/Some-tips-on-GradientBoosting/</url>
      <content type="html"><![CDATA[<h1 id="This-is-a-to-do-post"><a href="#This-is-a-to-do-post" class="headerlink" title="This is a to-do post!"></a>This is a to-do post!</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" target="_blank" rel="noopener">https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-gradient-descent-and-gradient-boosting" target="_blank" rel="noopener">Gradient Descent vs Residuals</a></li><li><a href="https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems" target="_blank" rel="noopener">Why does Gradient work so well</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Boosting </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Adaboost 算法分析</title>
      <link href="/2018/07/29/Adaboost-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/"/>
      <url>/2018/07/29/Adaboost-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>网上有很多Adaboost的算法介绍，也有很多数学推导，但是就我而言，我在查询这些资料的时候是不满意的，一方面，中文博客的数学推导有一些错误，影响读者理解，另一方面，英文博客数学公式要么没有要么对初学者很不友好，导致看的云里雾里，所以想写下这篇博客，里面有一些自己的思考，锻炼一下自己的耐性和写作能力。</p><p>我认为，一个成熟的机器学习算法，它的每个方面从数据集的选择，损失函数，优化等各方面都是有很深的学问的，所以我们在学习的时候万不能认为我们已经掌握了某个算法，其实深究下去，机器学习算法盘根错节，知识点相互关联，除非你是数学大牛，我认为轻易说出完全掌握这种话的同学相当不自量力。</p><p>和其他资料不同的是，我尝试着从最基本的Adaboost的定义出发，进而明确需要推导的量，最后根据公式推导，给出算法流程。我认为，这种思维模式有助于同学们思考一个算法需要考虑到那些问题。这是我的第一篇博客，写的当然没有网上的好，不过里面有我自己的思考，这篇博客还有会有改进的。</p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>Adaboost，全称是Adaptive Boosting,是Boosting算法家族里面比较典型的一种。所谓Boosting,用一句话来说，就是“三个臭皮匠，顶个诸葛亮”。这是什么意思呢，我们以分类器(Classifier)和二分类问题为例，如果我们获得了一些弱分类器，此处弱分类的的定义是，相较于随机的分类器，准确率只高出一点，我们能够利用这些弱分类器，组成强分类器。</p><p>以二分类问题为例，有数据集${(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，$y_i∈{+1,-1}$,Adaboost的强分类器的定义如下:$$H(x)=a_1h_1(x)+a_2h_2(x)+…+a_mh_m(x)\ a_i&gt;0,i=1,2,..,m$$<br>上式中，$h_i(x)​$ 为 第$i​$个分类器，以二分类为例，取值范围为${+1,-1}​$ ，$a_i​$为对应的权重，可知，最终的强分类器$H(x)​$ 是各个弱分类器的线性叠加。更准确的讲，最终的强分类器为:$$sign(H(x))​$$</p><p>Adaboost在决策树中应用最广泛，上述的弱分类器可以理解为只有一个分叉的决策树,基于Adaboost的决策树算法能有效的抵抗过拟合。</p><h1 id="相关公式的推导"><a href="#相关公式的推导" class="headerlink" title="相关公式的推导"></a>相关公式的推导</h1><p>本小节以二分类为例，介绍相关公式的推导。求解这个问题就是求解线性组合权重$a_i$,为了求解$a_i​$ ,首先介绍Adaboost的损失函数：$$L(a,h)=\sum_{i}^{N}exp(-y_iH(x_i))​$$至于为何选择指数作为损失函数，本文不做解释，实际上损失函数的选择是十分讲究的，涉及到很多方面，详细的分析可以参见文后的参考链接。但从表达式可以看出，如果分类正确的数量越多，损失越小，因此是合理的。损失函数展开如下：$$\begin{eqnarray}L(a,h) &amp;=&amp; \sum_{i}^{N}exp(-y_iH(x_i) \\\ &amp;=&amp; \sum_iexp(-y_iH_{m-1}(x))exp(-y_ia_mh_m(x)) \end{eqnarray}​$$ 令$\overline{w_{mi}}=exp(-y_iH_{m-1}(x_i))​$,则$$\begin{eqnarray}L(a,h) &amp;=&amp; \sum_{i}\overline{w_{mi}}exp(-y_{i}a_mh_m(x)) \nonumber  \\\ &amp;=&amp;\sum_{y_i=h_m(x_i)}\overline{w_{mi}}exp(-a_m)+\sum_{y_i\ne{h_m(x_i)}}\overline{w_{mi}}exp(a_m)\\\ &amp;=&amp; (e^{a_m}-e^{-a_{m}})\sum_{y_i \ne h_m(x_i)}\overline{w_{mi}}+e^{-a_m}\sum_{i}\overline{w_{mi}}\end{eqnarray} ​$$ 实际上，这个$\overline{w_{mi}}$是可以进行递推的，递推的结果为$$\overline{w_{mi}}=\overline{w_{m-1i}}exp(-y_ia_{m-1}h_{m-1i}(x_i))$$然后我们对$a_m​$求导，令导数为零，有如下结果$$a_m={1\over2}ln{1-e_m\over e_m}​$$其中，$e_m​=\frac{\sum_i\overline{w_mi}I(y_i\ne h_m(x_i))} {\sum_i\overline w_{mi}}$,关于这个量，仔细思考，发现是第$m$个分类器的误差率乘上对应的误差权重$\overline{w_{mi}}$，到现在为止，似乎我们已经把弱分类的线性权重给求出来了，还有很重要的一点被我们忽视了，我们在求导的时候，默认将$\overline{w_{mi}}$固定了，也就是说，我们默认将第$m$个分类器之前的所有分类器都安排的明明白白的，前$m-1$个分类器的权重都是最优的，这个时候我们求解第$m$个分类器的权重。</p><p>换句话说，前$m-1$个分类器权重都是最优的情况下，对第$m$个分类器也采用最优权重，能确保在解空间里面，这种方法求出的所有$m$个分类器的权重最优组合吗？答案是肯定的，我的理解是，这个问题可以理解为coordinate descent 问题，当然这涉及到另外的数学知识，感兴趣的可以查阅更多的资料。</p><p>整体而言，算法的推导还是比较简单的，但是这种简单只是一种表面现象，像上述的问题，进一步思考求证下去，能引出很多的子问题。文章的开头我就表达了这样一个观点，一个优秀的机器学习的算法背后隐藏着很深奥的学问，求实求真才是合格的学习态度。学习一个算法，不仅仅是记住算法，而应该尝试有自己的理解，用自己的理解来描述这个算法，记住一个算法可能只需要半天，但是欣赏算法中的数学美需要更长的功夫。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><ol><li>设有数据集${(x_1,y_1)…(x_N,y_N)}$，$m$个弱分类器${h_1…h_m}$</li><li>初始误差权重为$w_{1i}={1\over N}$</li><li>用$h_j$对数据集进行分类，有误差率$e_j$,进而算出$a_j$</li><li>更新权重，根据上节推到出来的递推公式$\overline{w_{ji}}=\overline{w_{j-1i}}exp(-y_ia_{j-1}h_{j-1i}(x_i))$，更新$\overline{w_{j+1i}}$,但是我们对误差权重$w_{j+1i}$进行归一化操作，让他变成一个概率分布</li><li>重复3-4直到算出所有的$a_j$</li><li>线性组合弱分类器，进而得到强分类器</li></ol><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>笔者有个倾向，在查阅资料的时候，特别喜欢看作者对算法的直觉理解，直觉不是幻觉，是一种“好像是这么回事”的感觉，这是建立在数学分析之上的，很可惜我发现英文资料里对直觉的分析比较多，中文资料很少，显得很“学院派”，上来就给读者分析数学，显得不有趣。那么我们来直觉上理解这个算法。</p><p>这个算法到底在干什么？假设我们刚进入了第$j$轮迭代，手上有了更新过的$\overline{w_{ji}}$,这个表示在第$j-1$轮迭代中分类器$H_{j-1}$对第$i$组数据犯错误的严重程度，如果这个数据在第$j-1$中被分类错误，那么第$j$轮中的权重会上升，（算法一开始的权重相等，$1 \over N$）这体现在本轮中的损失函数中，这个时候，求出$a_j$,这能够使得本轮对上轮的错误做出部分修正，然后进入第$j+1$轮。</p><p>假设我们完成了所有迭代，获得了$a_j$,意味着我们对所有分类错误的数据尽力完成了修正。也就是说，最后得到的强分类器中的每一个弱分类器都在修正前面弱分类器所犯下的错误。</p><p>后面的分类器尽力弥补前面分类器所犯错误的思想，是Boosting算法家族的共同点，比如将要介绍的Gradient Boosting算法。</p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>网上例子很多，比如<a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">这个</a></p><h1 id="Talk-is-cheap-show-me-your-code"><a href="#Talk-is-cheap-show-me-your-code" class="headerlink" title="Talk is cheap,show me your code"></a>Talk is cheap,show me your code</h1><p>未完待续</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这个算法的整理是按照我的理解进行的，难免有疏漏，公式推导显得粗糙，我也意识到了这一点，才疏学浅，将来会开放评论，欢迎大家斧正！</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://rob.schapire.net/papers/explaining-adaboost.pdf" target="_blank" rel="noopener">Adaboost Explained</a> 这篇论文很值得一读，从算法的简单性和对过拟合的分析出发，它解释了为何Adaboost是一个非常优秀的算法。</li><li><a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">https://blog.csdn.net/guyuealian/article/details/70995333</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
