<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>跟你的女朋友解释PCA</title>
      <link href="/2018/08/09/%E8%B7%9F%E4%BD%A0%E7%9A%84%E5%A5%B3%E6%9C%8B%E5%8F%8B%E8%A7%A3%E9%87%8APCA/"/>
      <url>/2018/08/09/%E8%B7%9F%E4%BD%A0%E7%9A%84%E5%A5%B3%E6%9C%8B%E5%8F%8B%E8%A7%A3%E9%87%8APCA/</url>
      <content type="html"><![CDATA[<p>Note: 本文假定读者和他的女朋友已经了解PCA(Principal Component Analysis)和FA(Factor Analysis)的数学原理，但是读者的女朋友需要一些更直观的理解，因此本文不对数学推导做详细解释。</p><p>为了表达的清晰，读者和他的女朋友打算统一符号:</p><ol><li>原数据集$X\in R^{n*m}$，表示$m$个$n$维属性，第$j$个属性表示为$N_j$</li><li>经过处理的后的数据集$Y\in R^{k*m}$，第$j$个属性表示为$K_j$，这些属性也就是主成分</li></ol><h3 id="PCA的作用？举一些例子？"><a href="#PCA的作用？举一些例子？" class="headerlink" title="PCA的作用？举一些例子？"></a>PCA的作用？举一些例子？</h3><p>不同于LDA(Linear Discriminant Analysis,线性判别分析)PCA是一种无监督的数据降维技术。在原始数据集中，通常属性与属性之间存在相关性，举个例子，在给跑车定价中，一辆跑车的属性有${百公里加速时间，最高时速，颜色，发动机….}$等等，显然百公里加速和最高时速存在一定的关系，也就是存在信息冗余，利用PCA可以降低数据冗余，同时很好的保留原有的数据信息。</p><p>PCA的应用场景有：</p><ol><li>数据压缩</li><li>在进行监督学习中，如果使用PCA对数据进行预处理，除了计算量下降外，有效减轻过拟合现象</li><li>PCA是一种降噪算法，在PCA中，我们一般只关心对数据多样性贡献较大的属性，忽略方差小的属性。</li></ol><h3 id="PCA属于feature-elimination-还是feature-extraction"><a href="#PCA属于feature-elimination-还是feature-extraction" class="headerlink" title="PCA属于feature elimination 还是feature extraction"></a>PCA属于feature elimination 还是feature extraction</h3><p>feature elimination 是直接把属性砍掉，这样虽然降低了数据的维度，但是损失了大量的信息,PCA属于feature extraction，PCA是将$n$个属性重新线性组合得到$k$个属性，</p><h3 id="PCA属性重组的标准是什么？"><a href="#PCA属性重组的标准是什么？" class="headerlink" title="PCA属性重组的标准是什么？"></a>PCA属性重组的标准是什么？</h3><p>PCA对属性重组有两个基本的标准：</p><ol><li>重组后的属性必须有较大的方差</li><li>重组后的属性必须能够尽可能的恢复原属性</li></ol><p>笔者在查阅资料的时候，思考了这两个标准之间的联系。感谢信息论老师，感谢学习到的信息论知识。下面就从信息论的角度直观地解释一下这两个标准的含义。</p><p>学过信息论就知道，作为衡量系统无序度的熵，如果一组数据方差小，就意味着信息熵小，极端的例子是，如果一组数据一模一样，那么信息熵就就是0，如果一组数据方差大，那么信息熵就大。</p><p>现在我们对数据进行压缩，追求的目标就是信息损失越小越好，如果PCA不满足第一个标准，即重组后的属性方差很小，那么我们就丢掉了大部分信息，这个时候我们不可能对数据进行重建。当然PCA是有损压缩(如果熟悉PCA的推导就知道，最后的变换矩阵不可逆)，但是如果重组后属性方差大，损失的信息就小，越容易重建原数据(当然完全重建是不可能的）</p><p>因此，这两个标准是内在统一的。<br><img src="https://i.stack.imgur.com/Q7HIP.gif"><br>例如上图，蓝点表示二维数据集，红点表示投影点也就是重组后的属性，紫红线表示原数据集方差最大的方向，大红线可以理解为误差，也就是损失的信息。中间的空心点表示均值。当然，你如果仔细看，会发现，重组后的属性方差越大，误差就越小，表明损失越小；重组后的属性方差越小，误差越大，表明损失的信息量太大了。你如果更仔细看会发现，这两个量的和是守恒的，23333</p><h3 id="数据集里的信息熵"><a href="#数据集里的信息熵" class="headerlink" title="数据集里的信息熵"></a>数据集里的信息熵</h3><p>首先要理解一点，一组数据最重要的是它的信息熵，也就是无序度，至于无序度以什么方式表现出来并不重要。</p><p>如果你不了解什么是信息熵，你不妨看看<a href="https://www.zhihu.com/question/22178202" target="_blank" rel="noopener">这里</a>。给定一个$X\in R^{n*m}$（$m$个$n$维向量）,$X$的无序度就已经确定了，不以人的定义为转移。这种无序度可能是显式的，可能就表现在那$n$维中；也有可能存在某些隐藏变量，这些隐藏变量的无序度通过$n$维属性表示出来。我们对数据进行处理，追求的目标就是减小信息熵的损耗，同时降低维度。</p><p>PCA在做什么呢，如果我们通过PCA得到了数据集$Y\in R^{k*m}$,如果在这$k$个新属性中的某个属性$K_j$有最大的方差，我们就认为$K_j$继承了大量的原数据集的信息熵，如此，我们能够同时达到减小信息熵消耗，降低维度的两个目的。</p><p>利用信息熵来理解数据降维技术能够给我们直观上的理解。再次感谢信息论，感谢香农。</p><p>使用信息熵及其相关理论的机器学习算法很多，比如在构建决策树的时候，我们用信息增益来决定树枝的分叉，用信息熵来衡量从父节点分叉后的平均纯净度；再比如如果一个网络输出的是概率，那么我们就可以使用交叉熵来构建损失函数。</p><h3 id="PCA为什么会涉及到特征值分解"><a href="#PCA为什么会涉及到特征值分解" class="headerlink" title="PCA为什么会涉及到特征值分解"></a>PCA为什么会涉及到特征值分解</h3><p>严格来讲，这涉及到公式推导的问题，在公式推导中有一个求最值的过程，这个时候涉及到特征值分解。当然也可以更加直观的理解，<a href="https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector" target="_blank" rel="noopener">这里</a>有个链接告诉你特征值和特征向量究竟是什么。</p><h3 id="PCA-V-S-FA"><a href="#PCA-V-S-FA" class="headerlink" title="PCA V.S. FA"></a>PCA V.S. FA</h3><p><a href="http://blog.sina.com.cn/s/blog_4b1b183d0100gz1b.html" target="_blank" rel="noopener">这里</a>的这篇文章写的很全面，我就不狗尾续貂了。</p><p>通过主成分分析所得来的新属性是原始属性的线性组合，每个主成分都是由原有$n$个属性线组合得到，在诸多主成分$K$中，$k_j$在总方差中占的比重最大，说明它综合原有变量的能力最强，其余主成分在总方差中占的比重依次递减，说明越往后的主成分综合原信息的能力越弱。以后的分析可以用前面几个方差最大的主成分来进行，一般情况下，要求前几个主成分所包含的信息不少于原始信息的85％ ，这样既减少了变量的数目，又能够用较少的主成分反映原有变量的绝大部分信息。如利用主成分来消除多元回归方程的多重共线性，利用主成分来筛选多元线性回归方程中的变量等。</p><p>通过因子分析得来的新变量是对每一个原始变量进行内部剖析。打比喻来说，原始变量就如成千上万的糕点，每一种糕点的原料都有面粉、油、糖及相应的不同原料，这其中，面粉、油、糖是所有糕点的共同材料，这正好象是因子分析中的新变量即因子变量。正确选择因子变量后，如果想考虑成千上万糕点的物价变动， 只需重点考虑面粉、油、糖等公共因子的物价变动即可。所以因子分析不是对原始变量的重新组合，而是对原始变量进行分解，分解为公共因子与特殊因子两部分。即因子分析就是要利用少数几个公共因子去解释较多个要观测变量中存在的复杂关系，它把原始变量分解为两部分因素，一部分是由所有变量共同具有的少数几个公共因子构成的，另一部分是每个原始变量独自具有的因素，即特殊因子。</p><p>具体而言：</p><ol><li>因子分析中是把属性表示成各因子的线性组合，而主成分分析中则是把主成分表示成各个属性的线性组合。 在主成分分析中,最终确定的新变量是原始变量的线性组合,如原始变量为${x_1,x_2,x_3…}$ ,经过坐标变换,将原有的${n}$个相关变量${x_i}$ 作线性变换,每个主成分都是由原有${n}$个变量线性组合得到。在诸多主成分${K_i}$ 中,$K_1$ 在方差中占的比重最大,说明它综合原有变量的能力最强,越往后主成分在方差中的比重也小,综合原信息的能力越弱。</li><li>主成分分析的重点在于解释个变量的总方差，而因子分析则把重点放在解释各变量之 间的协方差</li><li>主成分分析中不需要有假设(assumptions),因子分析则需要一些假设。因子分析的假设包括：各个共同因子之间不相关，特殊因子（specific factor）之间也不相关，共同因子和特殊因子之间也不相关。</li><li>主成分分析中，当给定的协方差矩阵或者相关矩阵的特征值是唯一的时候，的主成分 一般是独特的；而因子分析中因子不是独特的，可以旋转得到不到的因子。</li><li>在因子分析中，因子个数需要分析者指定（spss根据一定的条件自动设定，只要是特 征值大于1的因子进入分析），而指 定的因子数量不同而结果不同。在主成分分析中，成分的数量是一定的，一般有几个变量就有几个主成分。</li></ol><p>总的来说，主成分分析主要是作为一种探索性的技术，在分析者进行多元数据分析之前 ，用主成分分析来分析数据，让自己对数据有一个大致的了解是非常重要的。主成分分析一般很少单独使用：</p><ul><li>了解数据(screening the data)</li><li>和cluster analysis一 起使用</li><li>和判别分析一起使用，比如当变量很多，个案数不多，直接使用判别分析可能无解，这时候可以使用主成份发对变量简化。（reduce dimensionality）</li><li>在多元回归中，主成分分析可以帮助判断是否存在共线性（条件指数），还可以用来处理共线性。</li></ul><p>在算法上，主成分分析和因子分析很类似，不过，在因子分析中所采用的协方差矩阵的对角元素不再是变量的方差，而是和变量对应的共同度（变量方差中被各因子所解释的部分）。</p><hr><p>听完了你的解释，你的女朋友若有所思的点点头，连连夸你聪明，觉得你是世界上最厉害的。你也觉得这次分享很有必要，不仅加深了自己对PCA的理解，学习到的各种知识更加融会贯通了。但是此刻你们都觉得很累，毕竟刚刚进行了一场大脑风暴。你搂着女朋友沉沉地睡去了。</p><p>第二天醒来，你发现你并没有女朋友。</p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li><a href="https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector" target="_blank" rel="noopener">https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector</a></li><li><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues</a></li><li>cs229</li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Statistic </tag>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GLM 概览</title>
      <link href="/2018/08/03/GLM-%E6%A6%82%E8%A7%88/"/>
      <url>/2018/08/03/GLM-%E6%A6%82%E8%A7%88/</url>
      <content type="html"><![CDATA[<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><p>本文大部分总结自<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></p><h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>大家一开始接触回归问题的时候一定遇到过简单线性回归，Logistic Regression和Softmax问题，我在学习后两种回归问题的时候对方程的形式感觉很奇怪。比如：Logsitic Regression的方程(省略偏置项)如下：$$P(y=1|x)={1\over{1+e^{-w^Tx}}}$$$$P(y=0|x)={e^{-w^Tx}\over{1+e^{-w^Tx}}}$$为什么会是这种形式？这涉及到通用线性模型的概念(Generalized Linear Model,GLM),从GLM可以回答这个问题。</p><h1 id="GLM及其特化"><a href="#GLM及其特化" class="headerlink" title="GLM及其特化"></a>GLM及其特化</h1><p>GLM模型是建立在指数函数家族之上的,指数家族指的是下面这种形式的函数：$$P(y;\eta)=b(y)e^{\eta^TT(y)-a(\eta)}$$其中，各参数含义如下(这部分我不是特别清楚，从参考文献直接搬过来)：</p><ol><li>$\eta$为自然参数(Natural Parameter)</li><li>$T(y)$为sufficient statistic(下文考虑的分布通常为$T(y)=y$)</li><li>$a(\eta)$为归一化参量，用来使得算出的概率总和为1</li></ol><p>GLM做出如下假设：</p><ol><li>$y|x;\theta$ 服从某指数家族分布</li><li>讨论的目标为$E(y|x)$</li><li>$\eta$和$x$满足线性关系$\eta=w^Tx$</li></ol><p>简单线性回归，Logistic Regression 和Softmax问题都能通过方程变形变为GLM</p><h2 id="简单线性回归（以下简称线性回归，高斯分布）"><a href="#简单线性回归（以下简称线性回归，高斯分布）" class="headerlink" title="简单线性回归（以下简称线性回归，高斯分布）"></a>简单线性回归（以下简称线性回归，高斯分布）</h2><p>我们在处理线性回归问题的时候，通常如下建模：$$y=w^Tx+\epsilon$$其中$\epsilon$为误差项，一般认为$\epsilon\sim N(0,\delta^2)$,因为$\delta$的取值不影响讨论，不妨认为$\delta=1$,因此$y\sim N(w^Tx,1)$,因此$$\begin{eqnarray} p(y|x) &amp;=&amp; {1\over \sqrt{2\pi}}exp{(y-\mu)^2\over 2} \\\  &amp;=&amp; {1\over \sqrt{2\pi}} exp(-{y^2\over2})exp(\mu y-{\mu^2\over2})\end{eqnarray}$$所以$$\eta=\mu=w^Tx$$$$T(y)=y$$$$a(\eta)={\mu^2/2}$$ $$b(y)={1\over \sqrt{2\pi}}exp(-{y^2\over2})$$</p><h2 id="Logistic-Regression-伯努利分布"><a href="#Logistic-Regression-伯努利分布" class="headerlink" title="Logistic Regression(伯努利分布)"></a>Logistic Regression(伯努利分布)</h2><p>通过一定的变形，同样可以将Losgistic Regression 变形为指数分布，不详述。假设$p(y=1;\phi)=\phi$，结果如下<br>$$\eta=ln({\phi\over{1-\phi}})=&gt;\phi={1\over{1+e^{-\eta}}}$$ $$T(y)=y$$ $$a(\eta)=ln(1+e^\eta)$$ $$b(y)=1$$ 这个时候我们就发现，由于上述GLM的假设，自然得出$$P(y=1|x)={1\over{1+e^{-w^Tx}}}$$$$P(y=0|x)={e^{-w^Tx}\over{1+e^{-w^Tx}}}$$<br>这是相当奇妙的，我一开始接触到这个结论的时候很困惑，毕竟从伯努利分布得到Logistic Regression 这种形式还是有一定的跨度的，原来过程就藏在GLM模型里！</p><h3 id="Logistic-Regression-和高斯判定模型的联系"><a href="#Logistic-Regression-和高斯判定模型的联系" class="headerlink" title="Logistic Regression 和高斯判定模型的联系"></a>Logistic Regression 和高斯判定模型的联系</h3><p>此处岔开一段，和本文主题无关。实际上，Logistic Regression和高斯判定模型(Gaussian Discriminant Model)存在一些联系，简而言之，高斯判定模型是Logistic Regression的强化，高斯判定也是一种Logistic Regression。为了说明这种关系，定义符号如下:(假定读者熟悉多维高斯分布)</p><ol><li>$P(y=1)=\phi$</li><li>$\mu=\mu_0^{1-y}\mu_1^y,p(x|y=0,1)\sim e^{-{(x-\mu)^T\sum^{-1}(x-\mu)}\over2}$</li></ol><p>那么，我们要化简的公式就是：$p(y=1|x)$,如下：<br>$$\begin{eqnarray} p(y=1|x) &amp;=&amp;{p(y=1,x)\over p(x)} \\\ &amp;=&amp; {p(x|y=1)p(y=1)\over{\sum_i p(x|y=i)p(y=i)}} \end{eqnarray}$$ 读者将各式子带入并进行化简就能获得Logistic Regression公式的形式，笔者已经推导出来，因此应该是正确的。</p><p>那我们应该选择哪种判别模型呢，因为如果一个模型是高斯判定模型，那么它一定可以用Logistic Regression 进行拟合，反之不成立，因此，如果我们对模型有先验知识，认为该模型是高斯判定，那么就用高斯判定。</p><h2 id="Softmax（多项分布）"><a href="#Softmax（多项分布）" class="headerlink" title="Softmax（多项分布）"></a>Softmax（多项分布）</h2><p>其实仔细思考，伯努利分布其实是多项分布的特化，Logistic Regression 和Softmax的形式也有相似之处。由多项分布变形成GLM并不复杂，但是要借助一个小trick，即one hot编码，为了节省我的工作量，我就不写啦。详情见<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">26</a></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> GLM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Some tips on GradientBoosting</title>
      <link href="/2018/07/31/Some-tips-on-GradientBoosting/"/>
      <url>/2018/07/31/Some-tips-on-GradientBoosting/</url>
      <content type="html"><![CDATA[<h1 id="This-is-a-to-do-post"><a href="#This-is-a-to-do-post" class="headerlink" title="This is a to-do post!"></a>This is a to-do post!</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" target="_blank" rel="noopener">https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-gradient-descent-and-gradient-boosting" target="_blank" rel="noopener">Gradient Descent vs Residuals</a></li><li><a href="https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems" target="_blank" rel="noopener">Why does Gradient work so well</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Boosting </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Adaboost 算法分析</title>
      <link href="/2018/07/29/Adaboost-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/"/>
      <url>/2018/07/29/Adaboost-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>网上有很多Adaboost的算法介绍，也有很多数学推导，但是就我而言，我在查询这些资料的时候是不满意的，一方面，中文博客的数学推导有一些错误，影响读者理解，另一方面，英文博客数学公式要么没有要么对初学者很不友好，导致看的云里雾里，所以想写下这篇博客，里面有一些自己的思考，锻炼一下自己的耐性和写作能力。</p><p>我认为，一个成熟的机器学习算法，它的每个方面从数据集的选择，损失函数，优化等各方面都是有很深的学问的，所以我们在学习的时候万不能认为我们已经掌握了某个算法，其实深究下去，机器学习算法盘根错节，知识点相互关联，除非你是数学大牛，我认为轻易说出完全掌握这种话的同学相当不自量力。</p><p>和其他资料不同的是，我尝试着从最基本的Adaboost的定义出发，进而明确需要推导的量，最后根据公式推导，给出算法流程。我认为，这种思维模式有助于同学们思考一个算法需要考虑到那些问题。这是我的第一篇博客，写的当然没有网上的好，不过里面有我自己的思考，这篇博客还有会有改进的。</p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>Adaboost，全称是Adaptive Boosting,是Boosting算法家族里面比较典型的一种。所谓Boosting,用一句话来说，就是“三个臭皮匠，顶个诸葛亮”。这是什么意思呢，我们以分类器(Classifier)和二分类问题为例，如果我们获得了一些弱分类器，此处弱分类的的定义是，相较于随机的分类器，准确率只高出一点，我们能够利用这些弱分类器，组成强分类器。</p><p>以二分类问题为例，有数据集${(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，$y_i∈{+1,-1}$,Adaboost的强分类器的定义如下:$$H(x)=a_1h_1(x)+a_2h_2(x)+…+a_mh_m(x)\ a_i&gt;0,i=1,2,..,m$$<br>上式中，$h_i(x)​$ 为 第$i​$个分类器，以二分类为例，取值范围为${+1,-1}​$ ，$a_i​$为对应的权重，可知，最终的强分类器$H(x)​$ 是各个弱分类器的线性叠加。更准确的讲，最终的强分类器为:$$sign(H(x))​$$</p><p>Adaboost在决策树中应用最广泛，上述的弱分类器可以理解为只有一个分叉的决策树,基于Adaboost的决策树算法能有效的抵抗过拟合。</p><h1 id="相关公式的推导"><a href="#相关公式的推导" class="headerlink" title="相关公式的推导"></a>相关公式的推导</h1><p>本小节以二分类为例，介绍相关公式的推导。求解这个问题就是求解线性组合权重$a_i$,为了求解$a_i​$ ,首先介绍Adaboost的损失函数：$$L(a,h)=\sum_{i}^{N}exp(-y_iH(x_i))​$$至于为何选择指数作为损失函数，本文不做解释，实际上损失函数的选择是十分讲究的，涉及到很多方面，详细的分析可以参见文后的参考链接。但从表达式可以看出，如果分类正确的数量越多，损失越小，因此是合理的。损失函数展开如下：$$\begin{eqnarray}L(a,h) &amp;=&amp; \sum_{i}^{N}exp(-y_iH(x_i) \\\ &amp;=&amp; \sum_iexp(-y_iH_{m-1}(x))exp(-y_ia_mh_m(x)) \end{eqnarray}​$$ 令$\overline{w_{mi}}=exp(-y_iH_{m-1}(x_i))​$,则$$\begin{eqnarray}L(a,h) &amp;=&amp; \sum_{i}\overline{w_{mi}}exp(-y_{i}a_mh_m(x)) \nonumber  \\\ &amp;=&amp;\sum_{y_i=h_m(x_i)}\overline{w_{mi}}exp(-a_m)+\sum_{y_i\ne{h_m(x_i)}}\overline{w_{mi}}exp(a_m)\\\ &amp;=&amp; (e^{a_m}-e^{-a_{m}})\sum_{y_i \ne h_m(x_i)}\overline{w_{mi}}+e^{-a_m}\sum_{i}\overline{w_{mi}}\end{eqnarray} ​$$ 实际上，这个$\overline{w_{mi}}$是可以进行递推的，递推的结果为$$\overline{w_{mi}}=\overline{w_{m-1i}}exp(-y_ia_{m-1}h_{m-1i}(x_i))$$然后我们对$a_m​$求导，令导数为零，有如下结果$$a_m={1\over2}ln{1-e_m\over e_m}​$$其中，$e_m​=\frac{\sum_i\overline{w_mi}I(y_i\ne h_m(x_i))} {\sum_i\overline w_{mi}}$,关于这个量，仔细思考，发现是第$m$个分类器的误差率乘上对应的误差权重$\overline{w_{mi}}$，到现在为止，似乎我们已经把弱分类的线性权重给求出来了，还有很重要的一点被我们忽视了，我们在求导的时候，默认将$\overline{w_{mi}}$固定了，也就是说，我们默认将第$m$个分类器之前的所有分类器都安排的明明白白的，前$m-1$个分类器的权重都是最优的，这个时候我们求解第$m$个分类器的权重。</p><p>换句话说，前$m-1$个分类器权重都是最优的情况下，对第$m$个分类器也采用最优权重，能确保在解空间里面，这种方法求出的所有$m$个分类器的权重最优组合吗？答案是肯定的，我的理解是，这个问题可以理解为coordinate descent 问题，当然这涉及到另外的数学知识，感兴趣的可以查阅更多的资料。</p><p>整体而言，算法的推导还是比较简单的，但是这种简单只是一种表面现象，像上述的问题，进一步思考求证下去，能引出很多的子问题。文章的开头我就表达了这样一个观点，一个优秀的机器学习的算法背后隐藏着很深奥的学问，求实求真才是合格的学习态度。学习一个算法，不仅仅是记住算法，而应该尝试有自己的理解，用自己的理解来描述这个算法，记住一个算法可能只需要半天，但是欣赏算法中的数学美需要更长的功夫。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><ol><li>设有数据集${(x_1,y_1)…(x_N,y_N)}$，$m$个弱分类器${h_1…h_m}$</li><li>初始误差权重为$w_{1i}={1\over N}$</li><li>用$h_j$对数据集进行分类，有误差率$e_j$,进而算出$a_j$</li><li>更新权重，根据上节推到出来的递推公式$\overline{w_{ji}}=\overline{w_{j-1i}}exp(-y_ia_{j-1}h_{j-1i}(x_i))$，更新$\overline{w_{j+1i}}$,但是我们对误差权重$w_{j+1i}$进行归一化操作，让他变成一个概率分布</li><li>重复3-4直到算出所有的$a_j$</li><li>线性组合弱分类器，进而得到强分类器</li></ol><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>笔者有个倾向，在查阅资料的时候，特别喜欢看作者对算法的直觉理解，直觉不是幻觉，是一种“好像是这么回事”的感觉，这是建立在数学分析之上的，很可惜我发现英文资料里对直觉的分析比较多，中文资料很少，显得很“学院派”，上来就给读者分析数学，显得不有趣。那么我们来直觉上理解这个算法。</p><p>这个算法到底在干什么？假设我们刚进入了第$j$轮迭代，手上有了更新过的$\overline{w_{ji}}$,这个表示在第$j-1$轮迭代中分类器$H_{j-1}$对第$i$组数据犯错误的严重程度，如果这个数据在第$j-1$中被分类错误，那么第$j$轮中的权重会上升，（算法一开始的权重相等，$1 \over N$）这体现在本轮中的损失函数中，这个时候，求出$a_j$,这能够使得本轮对上轮的错误做出部分修正，然后进入第$j+1$轮。</p><p>假设我们完成了所有迭代，获得了$a_j$,意味着我们对所有分类错误的数据尽力完成了修正。也就是说，最后得到的强分类器中的每一个弱分类器都在修正前面弱分类器所犯下的错误。</p><p>后面的分类器尽力弥补前面分类器所犯错误的思想，是Boosting算法家族的共同点，比如将要介绍的Gradient Boosting算法。</p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>网上例子很多，比如<a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">这个</a></p><h1 id="Talk-is-cheap-show-me-your-code"><a href="#Talk-is-cheap-show-me-your-code" class="headerlink" title="Talk is cheap,show me your code"></a>Talk is cheap,show me your code</h1><p>未完待续</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这个算法的整理是按照我的理解进行的，难免有疏漏，公式推导显得粗糙，我也意识到了这一点，才疏学浅，将来会开放评论，欢迎大家斧正！</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://rob.schapire.net/papers/explaining-adaboost.pdf" target="_blank" rel="noopener">Adaboost Explained</a> 这篇论文很值得一读，从算法的简单性和对过拟合的分析出发，它解释了为何Adaboost是一个非常优秀的算法。</li><li><a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">https://blog.csdn.net/guyuealian/article/details/70995333</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
