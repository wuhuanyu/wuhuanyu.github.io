<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>GLM 概览</title>
      <link href="/2018/08/03/GLM-%E6%A6%82%E8%A7%88/"/>
      <url>/2018/08/03/GLM-%E6%A6%82%E8%A7%88/</url>
      <content type="html"><![CDATA[<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><p>本文大部分总结自<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></p><h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>大家一开始接触回归问题的时候一定遇到过简单线性回归，Logistic Regression和Softmax问题，我在学习后两种回归问题的时候对方程的形式感觉很奇怪。比如：Logsitic Regression的方程(省略偏置项)如下：$$P(y=1|x)={1\over{1+e^{-w^Tx}}}$$$$P(y=0|x)={e^{-w^Tx}\over{1+e^{-w^Tx}}}$$为什么会是这种形式？这涉及到通用线性模型的概念(Generalized Linear Model,GLM),从GLM可以回答这个问题。</p><h1 id="GLM及其特化"><a href="#GLM及其特化" class="headerlink" title="GLM及其特化"></a>GLM及其特化</h1><p>GLM模型是建立在指数函数家族之上的,指数家族指的是下面这种形式的函数：$$P(y;\eta)=b(y)e^{\eta^TT(y)-a(\eta)}$$其中，各参数含义如下(这部分我不是特别清楚，从参考文献直接搬过来)：</p><ol><li>$\eta$为自然参数(Natural Parameter)</li><li>$T(y)$为sufficient statistic(下文考虑的分布通常为$T(y)=y$)</li><li>$a(\eta)$为归一化参量，用来使得算出的概率总和为1</li></ol><p>GLM做出如下假设：</p><ol><li>$y|x;\theta$ 服从某指数家族分布</li><li>讨论的目标为$E(y|x)$</li><li>$\eta$和$x$满足线性关系$\eta=w^Tx$</li></ol><p>简单线性回归，Logistic Regression 和Softmax问题都能通过方程变形变为GLM</p><h2 id="简单线性回归（以下简称线性回归，高斯分布）"><a href="#简单线性回归（以下简称线性回归，高斯分布）" class="headerlink" title="简单线性回归（以下简称线性回归，高斯分布）"></a>简单线性回归（以下简称线性回归，高斯分布）</h2><p>我们在处理线性回归问题的时候，通常如下建模：$$y=w^Tx+\epsilon$$其中$\epsilon$为误差项，一般认为$\epsilon\sim N(0,\delta^2)$,因为$\delta$的取值不影响讨论，不妨认为$\delta=1$,因此$y\sim N(w^Tx,1)$,因此$$\begin{eqnarray} p(y|x) &amp;=&amp; {1\over \sqrt{2\pi}}exp{(y-\mu)^2\over 2} \\\  &amp;=&amp; {1\over \sqrt{2\pi}} exp(-{y^2\over2})exp(\mu y-{\mu^2\over2})\end{eqnarray}$$所以$$\eta=\mu=w^Tx$$$$T(y)=y$$$$a(\eta)={\mu^2/2}$$ $$b(y)={1\over \sqrt{2\pi}}exp(-{y^2\over2})$$</p><h2 id="Logistic-Regression-伯努利分布"><a href="#Logistic-Regression-伯努利分布" class="headerlink" title="Logistic Regression(伯努利分布)"></a>Logistic Regression(伯努利分布)</h2><p>通过一定的变形，同样可以将Losgistic Regression 变形为指数分布，不详述。假设$p(y=1;\phi)=\phi$，结果如下<br>$$\eta=ln({\phi\over{1-\phi}})=&gt;\phi={1\over{1+e^{-\eta}}}$$ $$T(y)=y$$ $$a(\eta)=ln(1+e^\eta)$$ $$b(y)=1$$ 这个时候我们就发现，由于上述GLM的假设，自然得出$$P(y=1|x)={1\over{1+e^{-w^Tx}}}$$$$P(y=0|x)={e^{-w^Tx}\over{1+e^{-w^Tx}}}$$<br>这是相当奇妙的，我一开始接触到这个结论的时候很困惑，毕竟从伯努利分布得到Logistic Regression 这种形式还是有一定的跨度的，原来过程就藏在GLM模型里！</p><h2 id="Softmax（多项分布）"><a href="#Softmax（多项分布）" class="headerlink" title="Softmax（多项分布）"></a>Softmax（多项分布）</h2><p>其实仔细思考，伯努利分布其实是多项分布的特化，Logistic Regression 和Softmax的形式也有相似之处。由多项分布变形成GLM并不复杂，但是要借助一个小trick，即one hot编码，为了节省我的工作量，我就不写啦。详情见<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">26</a></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> GLM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Some tips on GradientBoosting</title>
      <link href="/2018/07/31/Some-tips-on-GradientBoosting/"/>
      <url>/2018/07/31/Some-tips-on-GradientBoosting/</url>
      <content type="html"><![CDATA[<h1 id="This-is-a-to-do-post"><a href="#This-is-a-to-do-post" class="headerlink" title="This is a to-do post!"></a>This is a to-do post!</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" target="_blank" rel="noopener">https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-gradient-descent-and-gradient-boosting" target="_blank" rel="noopener">Gradient Descent vs Residuals</a></li><li><a href="https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems" target="_blank" rel="noopener">Why does Gradient work so well</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Boosting </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Adaboost 算法分析</title>
      <link href="/2018/07/29/Adaboost-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/"/>
      <url>/2018/07/29/Adaboost-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>网上有很多Adaboost的算法介绍，也有很多数学推导，但是就我而言，我在查询这些资料的时候是不满意的，一方面，中文博客的数学推导有一些错误，影响读者理解，另一方面，英文博客数学公式要么没有要么对初学者很不友好，导致看的云里雾里，所以想写下这篇博客，里面有一些自己的思考，锻炼一下自己的耐性和写作能力。</p><p>我认为，一个成熟的机器学习算法，它的每个方面从数据集的选择，损失函数，优化等各方面都是有很深的学问的，所以我们在学习的时候万不能认为我们已经掌握了某个算法，其实深究下去，机器学习算法盘根错节，知识点相互关联，除非你是数学大牛，我认为轻易说出完全掌握这种话的同学相当不自量力。</p><p>和其他资料不同的是，我尝试着从最基本的Adaboost的定义出发，进而明确需要推导的量，最后根据公式推导，给出算法流程。我认为，这种思维模式有助于同学们思考一个算法需要考虑到那些问题。这是我的第一篇博客，写的当然没有网上的好，不过里面有我自己的思考，这篇博客还有会有改进的。</p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>Adaboost，全称是Adaptive Boosting,是Boosting算法家族里面比较典型的一种。所谓Boosting,用一句话来说，就是“三个臭皮匠，顶个诸葛亮”。这是什么意思呢，我们以分类器(Classifier)和二分类问题为例，如果我们获得了一些弱分类器，此处弱分类的的定义是，相较于随机的分类器，准确率只高出一点，我们能够利用这些弱分类器，组成强分类器。</p><p>以二分类问题为例，有数据集${(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，$y_i∈{+1,-1}$,Adaboost的强分类器的定义如下:$$H(x)=a_1h_1(x)+a_2h_2(x)+…+a_mh_m(x)\ a_i&gt;0,i=1,2,..,m$$<br>上式中，$h_i(x)​$ 为 第$i​$个分类器，以二分类为例，取值范围为${+1,-1}​$ ，$a_i​$为对应的权重，可知，最终的强分类器$H(x)​$ 是各个弱分类器的线性叠加。更准确的讲，最终的强分类器为:$$sign(H(x))​$$</p><p>Adaboost在决策树中应用最广泛，上述的弱分类器可以理解为只有一个分叉的决策树,基于Adaboost的决策树算法能有效的抵抗过拟合。</p><h1 id="相关公式的推导"><a href="#相关公式的推导" class="headerlink" title="相关公式的推导"></a>相关公式的推导</h1><p>本小节以二分类为例，介绍相关公式的推导。求解这个问题就是求解线性组合权重$a_i$,为了求解$a_i​$ ,首先介绍Adaboost的损失函数：$$L(a,h)=\sum_{i}^{N}exp(-y_iH(x_i))​$$至于为何选择指数作为损失函数，本文不做解释，实际上损失函数的选择是十分讲究的，涉及到很多方面，详细的分析可以参见文后的参考链接。但从表达式可以看出，如果分类正确的数量越多，损失越小，因此是合理的。损失函数展开如下：$$\begin{eqnarray}L(a,h) &amp;=&amp; \sum_{i}^{N}exp(-y_iH(x_i) \\\ &amp;=&amp; \sum_iexp(-y_iH_{m-1}(x))exp(-y_ia_mh_m(x)) \end{eqnarray}​$$ 令$\overline{w_{mi}}=exp(-y_iH_{m-1}(x_i))​$,则$$\begin{eqnarray}L(a,h) &amp;=&amp; \sum_{i}\overline{w_{mi}}exp(-y_{i}a_mh_m(x)) \nonumber  \\\ &amp;=&amp;\sum_{y_i=h_m(x_i)}\overline{w_{mi}}exp(-a_m)+\sum_{y_i\ne{h_m(x_i)}}\overline{w_{mi}}exp(a_m)\\\ &amp;=&amp; (e^{a_m}-e^{-a_{m}})\sum_{y_i \ne h_m(x_i)}\overline{w_{mi}}+e^{-a_m}\sum_{i}\overline{w_{mi}}\end{eqnarray} ​$$ 实际上，这个$\overline{w_{mi}}$是可以进行递推的，递推的结果为$$\overline{w_{mi}}=\overline{w_{m-1i}}exp(-y_ia_{m-1}h_{m-1i}(x_i))$$然后我们对$a_m​$求导，令导数为零，有如下结果$$a_m={1\over2}ln{1-e_m\over e_m}​$$其中，$e_m​=\frac{\sum_i\overline{w_mi}I(y_i\ne h_m(x_i))} {\sum_i\overline w_{mi}}$,关于这个量，仔细思考，发现是第$m$个分类器的误差率乘上对应的误差权重$\overline{w_{mi}}$，到现在为止，似乎我们已经把弱分类的线性权重给求出来了，还有很重要的一点被我们忽视了，我们在求导的时候，默认将$\overline{w_{mi}}$固定了，也就是说，我们默认将第$m$个分类器之前的所有分类器都安排的明明白白的，前$m-1$个分类器的权重都是最优的，这个时候我们求解第$m$个分类器的权重。</p><p>换句话说，前$m-1$个分类器权重都是最优的情况下，对第$m$个分类器也采用最优权重，能确保在解空间里面，这种方法求出的所有$m$个分类器的权重最优组合吗？答案是肯定的，我的理解是，这个问题可以理解为coordinate descent 问题，当然这涉及到另外的数学知识，感兴趣的可以查阅更多的资料。</p><p>整体而言，算法的推导还是比较简单的，但是这种简单只是一种表面现象，像上述的问题，进一步思考求证下去，能引出很多的子问题。文章的开头我就表达了这样一个观点，一个优秀的机器学习的算法背后隐藏着很深奥的学问，求实求真才是合格的学习态度。学习一个算法，不仅仅是记住算法，而应该尝试有自己的理解，用自己的理解来描述这个算法，记住一个算法可能只需要半天，但是欣赏算法中的数学美需要更长的功夫。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><ol><li>设有数据集${(x_1,y_1)…(x_N,y_N)}$，$m$个弱分类器${h_1…h_m}$</li><li>初始误差权重为$w_{1i}={1\over N}$</li><li>用$h_j$对数据集进行分类，有误差率$e_j$,进而算出$a_j$</li><li>更新权重，根据上节推到出来的递推公式$\overline{w_{ji}}=\overline{w_{j-1i}}exp(-y_ia_{j-1}h_{j-1i}(x_i))$，更新$\overline{w_{j+1i}}$,但是我们对误差权重$w_{j+1i}$进行归一化操作，让他变成一个概率分布</li><li>重复3-4直到算出所有的$a_j$</li><li>线性组合弱分类器，进而得到强分类器</li></ol><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>笔者有个倾向，在查阅资料的时候，特别喜欢看作者对算法的直觉理解，直觉不是幻觉，是一种“好像是这么回事”的感觉，这是建立在数学分析之上的，很可惜我发现英文资料里对直觉的分析比较多，中文资料很少，显得很“学院派”，上来就给读者分析数学，显得不有趣。那么我们来直觉上理解这个算法。</p><p>这个算法到底在干什么？假设我们刚进入了第$j$轮迭代，手上有了更新过的$\overline{w_{ji}}$,这个表示在第$j-1$轮迭代中分类器$H_{j-1}$对第$i$组数据犯错误的严重程度，如果这个数据在第$j-1$中被分类错误，那么第$j$轮中的权重会上升，（算法一开始的权重相等，$1 \over N$）这体现在本轮中的损失函数中，这个时候，求出$a_j$,这能够使得本轮对上轮的错误做出部分修正，然后进入第$j+1$轮。</p><p>假设我们完成了所有迭代，获得了$a_j$,意味着我们对所有分类错误的数据尽力完成了修正。也就是说，最后得到的强分类器中的每一个弱分类器都在修正前面弱分类器所犯下的错误。</p><p>后面的分类器尽力弥补前面分类器所犯错误的思想，是Boosting算法家族的共同点，比如将要介绍的Gradient Boosting算法。</p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>网上例子很多，比如<a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">这个</a></p><h1 id="Talk-is-cheap-show-me-your-code"><a href="#Talk-is-cheap-show-me-your-code" class="headerlink" title="Talk is cheap,show me your code"></a>Talk is cheap,show me your code</h1><p>未完待续</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这个算法的整理是按照我的理解进行的，难免有疏漏，公式推导显得粗糙，我也意识到了这一点，才疏学浅，将来会开放评论，欢迎大家斧正！</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://rob.schapire.net/papers/explaining-adaboost.pdf" target="_blank" rel="noopener">Adaboost Explained</a> 这篇论文很值得一读，从算法的简单性和对过拟合的分析出发，它解释了为何Adaboost是一个非常优秀的算法。</li><li><a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">https://blog.csdn.net/guyuealian/article/details/70995333</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
